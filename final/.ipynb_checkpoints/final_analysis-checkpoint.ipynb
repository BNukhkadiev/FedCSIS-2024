{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63560d58-222d-456d-af53-d8fd72bc1171",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d08597-2eee-4f7f-8dcf-625be9cdb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)  # ensure reproducibility\n",
    "np.random.seed(0)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058b3b28-8b4f-4a6b-a04e-1a9887cb0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "\n",
    "def evaluate_error(preds, gt):\n",
    "    # Define the cost matrix\n",
    "    cost_matrix = np.array([[0, 1, 2], \n",
    "                            [1, 0, 1], \n",
    "                            [2, 1, 0]])\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(gt, preds)\n",
    "    \n",
    "    # Calculate the error value\n",
    "    err = np.sum(conf_matrix * cost_matrix) / len(gt)\n",
    "    \n",
    "    return err\n",
    "\n",
    "\n",
    "def get_scorer():\n",
    "    return make_scorer(evaluate_error,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c89dc7-093a-4ff0-8932-eec04d184a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    def __init__(self, val_size=0.25,\n",
    "                 encodeTarget=False,\n",
    "                 dummies=False,\n",
    "                 composite_features=False,\n",
    "                 scaler=\"standard\",\n",
    "                 imputer=\"simple\",\n",
    "                 only_use_compounds=False):\n",
    "        \n",
    "        # self.group_dictionary = pd.read_csv('data/group_dictionary.csv', sep=';')\n",
    "        self.test_data_no_target = pd.read_csv('../data/test_data_no_target.csv', sep=';', decimal=',')\n",
    "        self.training_data = pd.read_csv('../data/training_data.csv', sep=';', decimal=',')\n",
    "        # self.column_names_dictionary = pd.read_csv('data/column_names_dictionary.csv', sep=';')\n",
    "\n",
    "        if encodeTarget:\n",
    "            self.labelEncodeTarget()\n",
    "\n",
    "        if dummies:\n",
    "            self.makeDummies()\n",
    "        \n",
    "        self.X = self.training_data.drop(columns=['Class','Perform'])  \n",
    "        self.y = self.training_data[['Class']]\n",
    "\n",
    "        \n",
    "        # collect cat_cols, bin_cols, num_cols\n",
    "        # num_cols are then used for scaling\n",
    "        # cat_cols are gonna be encoded\n",
    "        self.cat_cols = []\n",
    "        self.bin_cols = []\n",
    "        self.num_cols = []\n",
    "        for col in self.X.columns.tolist():\n",
    "            if len(self.X[col].value_counts()) > 2 and len(self.X[col].value_counts())<10:\n",
    "                self.cat_cols.append(col)\n",
    "            elif len(self.X[col].value_counts()) == 2:\n",
    "                self.bin_cols.append(col)\n",
    "            else:\n",
    "                self.num_cols.append(col)\n",
    "\n",
    "    \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X, self.y, test_size=val_size, random_state=42)\n",
    "\n",
    "        if imputer == \"simple\":\n",
    "            self.simpleImpute(0)\n",
    "\n",
    "        if composite_features == True:\n",
    "            self.create_composite_features()\n",
    "            for col in self.X.columns.tolist():\n",
    "                if len(self.X[col].value_counts())>10:\n",
    "                    self.num_cols.append(col)\n",
    "        \n",
    "        if scaler == \"standard\":\n",
    "            self.standardScale()\n",
    "        \n",
    "        elif scaler == \"minmax\":\n",
    "            self.minMaxScale()\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_X_train(self):\n",
    "        return self.X_train\n",
    "\n",
    "    \n",
    "    def get_y_train(self):\n",
    "        return self.y_train\n",
    "\n",
    "    \n",
    "    def get_X_val(self):\n",
    "        return self.X_val\n",
    "\n",
    "    \n",
    "    def get_y_val(self):\n",
    "        return self.y_val\n",
    "\n",
    "    \n",
    "    def makeDummies(self):\n",
    "        self.training_data = pd.concat((self.training_data.drop(columns=['Group']), pd.get_dummies(self.training_data['Group'])), axis=1)\n",
    "        self.test_data_no_target = pd.concat((self.test_data_no_target.drop(columns=['Group']), pd.get_dummies(self.test_data_no_target['Group'])), axis=1)\n",
    "        \n",
    "\n",
    "    def knnImpute(self):\n",
    "        self.knnImputer = KNNImputer(n_neighbors=4)\n",
    "        self.X_train[self.num_cols] = self.knnImputer.fit_transform(self.X_train[self.num_cols])\n",
    "        self.X_val[self.num_cols] = self.knnImputer.transform(self.X_val[self.num_cols])\n",
    "        self.test_data_no_target[self.num_cols] = self.knnImputer.transform(self.test_data_no_target[self.num_cols])\n",
    "\n",
    "\n",
    "    def simpleImpute(self, value):\n",
    "        self.simpleImputer = SimpleImputer(strategy='mean')\n",
    "        self.X_train[self.num_cols] = self.simpleImputer.fit_transform(self.X_train[self.num_cols])\n",
    "        self.X_val[self.num_cols] = self.simpleImputer.transform(self.X_val[self.num_cols])\n",
    "        self.test_data_no_target[self.num_cols] = self.simpleImputer.transform(self.test_data_no_target[self.num_cols])\n",
    "\n",
    "\n",
    "    def standardScale(self):\n",
    "        self.stdScaler = StandardScaler()\n",
    "        self.X_train[self.num_cols] = self.stdScaler.fit_transform(self.X_train[self.num_cols])\n",
    "        self.X_val[self.num_cols] = self.stdScaler.transform(self.X_val[self.num_cols])\n",
    "        self.test_data_no_target[self.num_cols] = self.stdScaler.transform(self.test_data_no_target[self.num_cols])\n",
    "        \n",
    "\n",
    "    def minMaxScale(self):\n",
    "        self.minMaxScale = MinMaxScaler()\n",
    "        self.X_train[self.num_cols] = self.minMaxScale.fit_transform(self.X_train[self.num_cols])\n",
    "        self.X_val[self.num_cols] = self.minMaxScale.transform(self.X_val[self.num_cols])\n",
    "        self.test_data_no_target[self.num_cols] = self.minMaxScale.transform(self.test_data_no_target[self.num_cols])\n",
    "\n",
    "    \n",
    "    def PCASelection(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def labelEncodeTarget(self):\n",
    "        # label encodes target to be 0, 1, 2\n",
    "        self.target_encoder = LabelEncoder()\n",
    "        self.training_data['Class'] = self.training_data[['Class']].apply(self.target_encoder.fit_transform)\n",
    "        \n",
    "\n",
    "    def labelDecodeTarget(self, data):\n",
    "        # returns python list decoding back to -1, 0, 1\n",
    "        return self.target_encoder.inverse_transform(data.ravel()).tolist()\n",
    "\n",
    "\n",
    "    def getCompundFeatures(self):\n",
    "        if create_composite_features:\n",
    "            self.create_composite_features()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def create_composite_features(self):\n",
    "        # Liquidity Ratios\n",
    "        self.X_train['Liquidity Ratio'] = self.X_train[['I49', 'I50', 'I52']].mean(axis=1)\n",
    "        self.X_val['Liquidity Ratio'] = self.X_val[['I49', 'I50', 'I52']].mean(axis=1)\n",
    "        self.test_data_no_target['Liquidity Ratio'] = self.test_data_no_target[['I49', 'I50', 'I52']].mean(axis=1)\n",
    "        \n",
    "        # Efficiency Ratios\n",
    "        self.X_train['Efficiency Ratio'] = self.X_train[['I21', 'I22', 'I24', 'I25', 'I28']].mean(axis=1)\n",
    "        self.X_val['Efficiency Ratio'] = self.X_val[['I21', 'I22', 'I24', 'I25', 'I28']].mean(axis=1)\n",
    "        self.test_data_no_target['Efficiency Ratio'] = self.test_data_no_target[['I21', 'I22', 'I24', 'I25', 'I28']].mean(axis=1)\n",
    "        \n",
    "        # Calculate Total Equity\n",
    "        self.X_train['Total Equity'] = self.X_train['I19'] / self.X_train['I53']\n",
    "        self.X_val['Total Equity'] = self.X_val['I19'] / self.X_val['I53']\n",
    "        self.test_data_no_target['Total Equity'] = self.test_data_no_target['I19'] / self.test_data_no_target['I53']\n",
    "\n",
    "        # Calculate Total Assets\n",
    "        self.X_train['Total Assets'] = self.X_train['Total Equity'] / (1 - self.X_train['I54'])\n",
    "        self.X_val['Total Assets'] = self.X_val['Total Equity'] / (1 - self.X_val['I54'])\n",
    "        self.test_data_no_target['Total Assets'] = self.test_data_no_target['Total Equity'] / (1 - self.test_data_no_target['I54'])\n",
    "\n",
    "        # Calculate Equity Multiplier\n",
    "        self.X_train['Equity Multiplier'] = self.X_train['Total Assets'] / self.X_train['Total Equity']\n",
    "        self.X_val['Equity Multiplier'] = self.X_val['Total Assets'] / self.X_val['Total Equity']\n",
    "        self.test_data_no_target['Equity Multiplier'] = self.test_data_no_target['Total Assets'] / self.test_data_no_target['Total Equity']\n",
    "\n",
    "        # Leverage Ratios\n",
    "        self.X_train['Leverage Ratio'] = self.X_train[['I17', 'I19', 'I55', 'I54', 'Equity Multiplier']].mean(axis=1)\n",
    "        self.X_val['Leverage Ratio'] = self.X_val[['I17', 'I19', 'I55', 'I54', 'Equity Multiplier']].mean(axis=1)\n",
    "        self.test_data_no_target['Leverage Ratio'] = self.test_data_no_target[['I17', 'I19', 'I55', 'I54', 'Equity Multiplier']].mean(axis=1)\n",
    "        \n",
    "        # Profitability Ratios\n",
    "        self.X_train['Profitability Ratio'] = self.X_train[['I1', 'I2', 'I6', 'I11', 'I34', 'I35', 'I37', 'I32', 'I33', 'I38']].mean(axis=1)\n",
    "        self.X_val['Profitability Ratio'] = self.X_val[['I1', 'I2', 'I6', 'I11', 'I34', 'I35', 'I37', 'I32', 'I33', 'I38']].mean(axis=1)\n",
    "        self.test_data_no_target['Profitability Ratio'] = self.test_data_no_target[['I1', 'I2', 'I6', 'I11', 'I34', 'I35', 'I37', 'I32', 'I33', 'I38']].mean(axis=1)\n",
    "\n",
    "        # Price Ratios\n",
    "        self.X_train['Price Ratios'] = self.X_train[['I41', 'I42', 'I43', 'I56', 'I58', 'I44']].mean(axis=1)\n",
    "        self.X_val['Price Ratios'] = self.X_val[['I41', 'I42', 'I43', 'I56', 'I58', 'I44']].mean(axis=1)\n",
    "        self.test_data_no_target['Price Ratios'] = self.test_data_no_target[['I41', 'I42', 'I43', 'I56', 'I58', 'I44']].mean(axis=1)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadec915-c4c0-4a56-989c-49bfc3028af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataTransformer(encodeTarget=True, dummies=True, composite_features=False)\n",
    "X_train = data.get_X_train()\n",
    "X_val = data.get_X_val()\n",
    "y_train = data.get_y_train()\n",
    "y_val = data.get_y_val()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543bff2-be6a-4b02-b4d3-5de3eaff9f4c",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861157fc-1b58-4cb2-8053-8b320b285060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0000\n",
      "Epoch [2/10], Loss: 0.0000\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# final approach will be neural network with compounds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have preprocessed X_train and y_train as numpy arrays or torch tensors\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  # Assuming classification, adjust dtype if needed\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your neural network model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define your model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]  # Assuming X_train is a 2D array\n",
    "hidden_size = 128  # You can adjust this as needed\n",
    "num_classes = len(set(y_train))  # Assuming y_train contains class labels\n",
    "model = MyModel(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # You can adjust this as needed\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    custom_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # custom_loss += evaluate_error(out)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f731e2-e6b1-48cf-bd98-cdd6a4792d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
